{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c8ee9-c0ac-4c8c-942b-2b519492cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7470734-567d-4d0b-b22a-1cecb3b07148",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5d726-d022-4ed4-945d-c9d915e64364",
   "metadata": {},
   "source": [
    "We covered some popular object detectors in the lectures. Due to limitations on computational resources, in this notebook we will not define or train our own object detector. Instead, we will take a pretrained Faster R-CNN from PyTorch's TorchVision library and use it as an example to illustrate how detectors work. Check out PyTorch's [online docs](https://pytorch.org/vision/stable/models.html) for more information on pretrained models provided by TorchVision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41fc30-6483-4150-9abd-7794318d655a",
   "metadata": {},
   "source": [
    "## Running a Pretrained Object Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4fa75-4530-4bde-8913-ffb66d8008de",
   "metadata": {},
   "source": [
    "We will use the `fasterrcnn_resnet50_fpn_v2` model from TorchVision. It is a [Faster R-CNN](https://arxiv.org/abs/1506.01497) model with a 50-layer [ResNet](https://arxiv.org/abs/1512.03385) as its backbone. A [Feature Pyramid Network](https://arxiv.org/abs/1612.03144) is used to make the model better at detecting objects at multiple scales. The model is trained on the [COCO dataset](https://cocodataset.org/) and is able to detect objects from [80 categories](https://cocodataset.org/#explore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66700e9-c7f6-4739-8853-01a2a93c6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "\n",
    "# Create model and load the pretrained weights.\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformations used to preprocess input images.\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07fad72-b2a7-4a1d-a192-ffadaae39eeb",
   "metadata": {},
   "source": [
    "We will use the following image from COCO as our sample image. Don't worry. This image is from the validation set, so the model has not seen it during training. (You could argue that there is still risk for overfitting, since the hyperparameters are tuned using this validation set. That's very true. We will still use this image for illustrative purposes, since it is easier to get the annotations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752a0f9-f1ff-4412-b4b4-d01db6e62e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('data/000000570688.jpg')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6876bfc-7dd0-4e44-92fe-d88c567eee86",
   "metadata": {},
   "source": [
    "Let's run the model our sample image,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42d5ed-645e-430d-af54-1c3096c1857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the sample image.\n",
    "inp = torch.tensor(np.transpose(img, (2, 0, 1)))\n",
    "batch = [preprocess(inp)]\n",
    "with torch.no_grad():\n",
    "    prediction = model(batch)[0]\n",
    "category_map = weights.meta['categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bc965-af57-4600-bfe6-326a4ec111be",
   "metadata": {},
   "source": [
    "... and display the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d06b6-a0a4-416e-b406-982ceb9490e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def visualize_detections(img, prediction, category_map, ground_truth=None,\n",
    "    score_thresh=0.5, categories=None, display_text=False):\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Visualize detections.\n",
    "    for i in range(len(prediction['boxes'])):\n",
    "        x1, y1, x2, y2 = prediction['boxes'][i].detach().cpu().numpy() \n",
    "        label = prediction['labels'][i].item()\n",
    "        score = prediction['scores'][i].item()\n",
    "        category = category_map[label]\n",
    "        if score > score_thresh:\n",
    "            if categories and category not in categories:\n",
    "                continue\n",
    "            plt.gca().add_patch(Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                facecolor='none', edgecolor=(1,0,0), linewidth=1))\n",
    "            if display_text:\n",
    "                plt.text(x1+4., y1-6., f'{category} {score:.3f}',\n",
    "                    backgroundcolor=(1,0,0,0.5))\n",
    "    \n",
    "    # Visualize ground-truth.\n",
    "    if ground_truth:\n",
    "        for i in range(len(ground_truth['boxes'])):\n",
    "            x1, y1, x2, y2 = ground_truth['boxes'][i]\n",
    "            label = ground_truth['labels'][i]\n",
    "            category = category_map[label]\n",
    "            if categories and category not in categories:\n",
    "                continue\n",
    "            plt.gca().add_patch(Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                    facecolor='none', edgecolor=(0,0,1), linewidth=1))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "visualize_detections(img, prediction, category_map, score_thresh=0.9, display_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e23e8f-7ca1-455c-a3e0-41b0e2b38797",
   "metadata": {},
   "source": [
    "## Evaluating the Detector's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09ab61-fb59-4750-b040-59dcdb5e5c93",
   "metadata": {},
   "source": [
    "It looks like our detector is doing reasonably well on this image. Let's see how we can evaluate its performance. To make things simpler, we will focus on the \"kite\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c2de8-465c-4b8a-a4ce-ba299b0fe8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_kites = np.array([\n",
    "    [ 58.00, 255.00,  70.00, 261.00],\n",
    "    [176.80, 237.98, 257.07, 258.72],\n",
    "    [331.47,  64.73, 389.83,  85.99],\n",
    "    [281.05, 216.76, 422.40, 265.44],\n",
    "    [222.76, 265.81, 395.02, 323.61],\n",
    "    [240.93, 124.85, 264.28, 136.92],\n",
    "    [431.24,  29.56, 450.01,  44.34],\n",
    "    [116.24, 178.44, 136.90, 191.40],\n",
    "    [232.72,  20.46, 242.42,  30.16],\n",
    "    [221.00, 179.65, 453.09, 241.63],\n",
    "    [461.84, 252.38, 480.25, 269.08],\n",
    "    [506.64, 195.03, 523.44, 215.88],\n",
    "    [323.96, 160.00, 465.92, 215.13],\n",
    "    [280.32,  92.35, 326.35, 106.77],\n",
    "])\n",
    "ground_truth = {\n",
    "    'boxes': gt_kites,\n",
    "    'labels': [38 for _ in range(len(gt_kites))]\n",
    "}\n",
    "visualize_detections(img, prediction, category_map, ground_truth=ground_truth,\n",
    "    categories=['kite'], score_thresh=0.9, display_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975146c-703a-411f-9ef4-d4b686dab293",
   "metadata": {},
   "source": [
    "In this visualization, detected boxes are marked red, whereas ground-truth boxes are marked blue. You might first notice that some detected kites are not in the ground-truth annotations. This is because in COCO's annotation protocol, scenes like this are considered \"crowd\" and the annotations may not be exhaustive. \"Crowd\" areas are ignored when calculating the final metric so they do not negatively affect the evaluation. Here let's just assume the annotations are correct.\n",
    "\n",
    "We picked a relatively high score threshold in this visualization (`score_thresh=0.9`). Generally speacking, a higher score threshold means your results will have higher *precision*, at the expense of lower *recall*. Precision is defined as the number of correct predictions divided by the total number of predictions. Recall is defined as the number of correct predictions divided by the total number of ground-truth objects. As a result of the high score threshold, some kites, like the one in the middle near the top of the image, are missed by our detector.\n",
    "\n",
    "If we pick a lower score threshold, we could possibly recover the missed objects, resulting in higher recall. However, this may introduce more false positives. In the visualization below, the previously missed kite in the middle is now detected, but a false positive bounding box encompassing two kites also appears around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9ff8e-6510-4e6b-a6ce-548f4169dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_detections(img, prediction, category_map, ground_truth=ground_truth,\n",
    "    categories=['kite'], score_thresh=0.7, display_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f6235-5cb3-42dd-ba57-9adab4ca5026",
   "metadata": {},
   "source": [
    "When evaluating the detector, we can choose to plot the *precision-recall* curve instead of picking one specific score threshold. To do this, we first need to sort the predictions based on their confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ae31d-3ed7-47f4-a050-a338d88e50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_kites = []\n",
    "dt_scores = []\n",
    "for i in range(len(prediction['boxes'])):\n",
    "    if prediction['labels'][i].item() == 38:\n",
    "        dt_kites.append(prediction['boxes'][i].detach().cpu().numpy())\n",
    "        dt_scores.append(prediction['scores'][i].item())\n",
    "dt_kites = np.array(dt_kites)\n",
    "dt_scores = np.array(dt_scores)\n",
    "\n",
    "idxs = np.argsort(-dt_scores)\n",
    "dt_kites = dt_kites[idxs]\n",
    "dt_scores = dt_scores[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945a164-8aa1-40f5-b1e8-fc243909597a",
   "metadata": {},
   "source": [
    "Whether a prediction is a true positive or a false positive depends on how well it overlaps with the ground-truth bounding boxes. Recall from the lecture that this is usually measured by calculating the IoU (intersection over union) between bounding boxes.\n",
    "\n",
    "(10 points) In the cell below, implement function `calculate_ious()`. It receives two bounding box lists, and returns a matrix `ious` where `ious[i, j]` is the IoU between `boxes1[i]` and `boxes2[j]`. Try to implement this in vectorized form without using for-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44770d37-df0f-4f8b-8f96-6c302886925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ious(boxes1, boxes2):\n",
    "    \"\"\"Calculate IoU (intersection over union) between two bounding box lists.\n",
    "    \n",
    "    Args:\n",
    "    - boxes1: M x 4 array representing M bounding boxes in the\n",
    "        [left, top, right, bottom] format.\n",
    "    - boxes2: N x 4 array representing N bounding boxes in the\n",
    "        [left, top, right, bottom] format.\n",
    "    \n",
    "    Returns:\n",
    "    - ious: M x N array where ious[i, j] is the IoU between boxes1[i] and\n",
    "        boxes2[j].\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return ious\n",
    "\n",
    "# Calculate IoUs and visualize.\n",
    "ious = calculate_ious(dt_kites, gt_kites)\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "plt.matshow(ious)\n",
    "plt.xlabel('ground-truths')\n",
    "plt.ylabel('detections')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "plt.colorbar(cax=cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c28156e-9b8d-466e-9f07-e675c4323ee9",
   "metadata": {},
   "source": [
    "With the IoU values we can plot the precision-recall curve. First, we take the highest scoring detection, compare it with all ground-truth bounding boxes and check if the highest IoU is over a threshold. The detection is labeled a true positive if so and a false positive otherwise. We then move on to the second highest scoring detection, and try to match it to the remaining ground-truth boxes. Repeat this process when there is no remaining detection or ground-truth. After every detection is labeled, we can calculate the precision and recall values and plot the curve accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803482c-e1fc-441c-a48d-fe517480df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_threshold = 0.5\n",
    "tp_labels = []\n",
    "# The detections is already sorted by confidence scores.\n",
    "for i in range(len(dt_kites)):\n",
    "    # Find the ground-truth that overlap the most.\n",
    "    j = np.argmax(ious[i, :])\n",
    "    if ious[i, j] > iou_threshold:\n",
    "        # True positive if there is a match.\n",
    "        tp_labels.append(True)\n",
    "        # Remove matched ground-truth.\n",
    "        ious[:, j] = -np.inf\n",
    "    else:\n",
    "        # False positive if there is no match.\n",
    "        tp_labels.append(False)\n",
    "\n",
    "# Calculate precision and recall values.\n",
    "pr = []\n",
    "rc = []\n",
    "tp, fp, n_gt = 0, 0, len(gt_kites)\n",
    "for l in tp_labels:\n",
    "    if l:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "    pr.append(tp / (tp + fp))\n",
    "    rc.append(tp / n_gt)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(rc, pr, '.-', color='b', clip_on=False)\n",
    "plt.grid()\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0dc9f5-80a6-44db-9ca4-b0e7756ea806",
   "metadata": {},
   "source": [
    "## Anchor Boxes and Region Proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb8cae-8d13-4dcb-a7e6-6cee6bb641e2",
   "metadata": {},
   "source": [
    "Recall from the lecture that the R-CNN series of detectors convert the problem of detecting objects in images into sampling regions of interest (RoIs) and classifying (and adjusting) them. Faster R-CNN is usually described as a \"two-stage\" detector. In the first stage, the detector's region proposal network (RPN) samples a dense grid of boxes called \"anchors\", predicts the \"objectness\" scores for each anchor, and uses regression to adjust the locations of the anchors. Top scoring anchors are used as region proposals and passed to the second stage, where the classification head classifies their object category while the regression head further adjusts their locations.\n",
    "\n",
    "In this section, we will take a look at what these anchors look like. PyTorch let's you fetch intermediate results in a model by registering hooks on its modules. In the cell below, we define some necessary hooks and run inference on the sample image again with these hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2f281-8747-453b-9cb7-9a9d3ff8d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "\n",
    "def get_output_hook(name):\n",
    "    def hook_fn(module, inp, out):\n",
    "        outputs[name] = out\n",
    "    return hook_fn\n",
    "\n",
    "hooked_modules = {\n",
    "    'transform': model.transform,\n",
    "    'anchors': model.rpn.anchor_generator,\n",
    "    'rpn': model.rpn,\n",
    "    'backbone': model.backbone,\n",
    "}\n",
    "\n",
    "handles = []\n",
    "for name, module in hooked_modules.items():\n",
    "    handles.append(module.register_forward_hook(get_output_hook(name)))\n",
    "with torch.no_grad():\n",
    "    prediction = model(batch)[0]\n",
    "for handle in handles:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff73a9-43da-4f7d-a3c1-cf6f79466e85",
   "metadata": {},
   "source": [
    "Before processing the image, the detector applies some transformations to the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a22701-3238-4619-98d5-4c103bcb2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.transform)\n",
    "img_transformed = outputs['transform'][0].tensors.detach().cpu().numpy()[0]\n",
    "print(img_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77264e94-fc92-48fd-b35f-bdc53e1fb24f",
   "metadata": {},
   "source": [
    "The backbone of the detector contains a feature pyramid network that produces feature maps at multiple scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7826b7-b83b-4d27-afdb-470e7a59250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, feature_map in outputs['backbone'].items():\n",
    "    print(f'{name:<4}: {feature_map.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5c00f-2ef9-49cc-aed0-151cf46b666e",
   "metadata": {},
   "source": [
    "Anchors are dense grids of box samples on these feature maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d85cd-1d81-4e9f-bb17-a264844fac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sizes = []\n",
    "for feature_map in outputs['backbone'].values():\n",
    "    n, c, h, w = feature_map.shape\n",
    "    grid_sizes.append([h, w])\n",
    "print(grid_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a9a01-9346-4940-81c8-7249d97b58b1",
   "metadata": {},
   "source": [
    "When defining the detector, you need to specify the size of anchors corresponding to each feature map and the aspect ratios to be used. In our model here, it generates three bounding boxes for each grid point, their aspect ratios being 0.5, 1 and 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07aa705-1986-4134-8581-c16947891c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.rpn.anchor_generator.sizes)\n",
    "print(model.rpn.anchor_generator.aspect_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aceb493-0b9e-4242-838e-3692005e1b0c",
   "metadata": {},
   "source": [
    "Let's visualize some anchors on the last grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4c7f7-5dfe-4737-a3ba-db2521d7f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = outputs['anchors'][0].detach().cpu().numpy()\n",
    "\n",
    "plt.imshow(img_transformed[0], cmap='gray')\n",
    "# Show grid.\n",
    "n = grid_sizes[-1][0]*grid_sizes[-1][1]\n",
    "x1, y1, x2, y2 = np.split(anchors[-3*n::3], 4, axis=1)\n",
    "xc = (x1 + x2) / 2\n",
    "yc = (y1 + y2) / 2\n",
    "plt.scatter(xc, yc, color=(0,0,1), s=10)\n",
    "# Show bounding boxes of different aspect ratios on one grid point.\n",
    "for x1, y1, x2, y2 in anchors[-3*(n//2):-3*(n//2)+3]:\n",
    "    plt.scatter((x1 + x2) / 2, (y1 + y2) / 2, color=(1,0,0), s=10)\n",
    "    plt.gca().add_patch(Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "        facecolor='none', edgecolor=(1,0,0), linewidth=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebcd07c-346c-407a-8c04-891f3718aa75",
   "metadata": {},
   "source": [
    "The RPN predicts objectness scores for each anchor and keeps the highest scoring ones. In our detector, 1000 are kept and passed to the RoI prediction heads as possible object regions. Some of them are visualized in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea83c1d-ecb5-4b2c-97ba-507ea38bae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = outputs['rpn'][0][0].detach().cpu().numpy()\n",
    "print(rois.shape)\n",
    "\n",
    "plt.imshow(img_transformed[0], cmap='gray')\n",
    "for x1, y1, x2, y2 in rois[:200]:\n",
    "    plt.gca().add_patch(Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "        facecolor='none', edgecolor=(1,0,0), linewidth=1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
