{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93305187-434c-4c1a-8ca1-0c989171b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893f571-9f90-41d6-9376-2b8303fb976d",
   "metadata": {},
   "source": [
    "# PyTorch Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f29c1-3f70-4a1d-abf7-2c756072ba2d",
   "metadata": {},
   "source": [
    "We will be using PyTorch in the notebooks for the next several modules. In this notebook, we will go over some of the basic notions and operations in PyTorch. You may find it helpful to check out the [official guide](https://pytorch.org/tutorials/beginner/basics/intro.html) for PyTorch, from which many examples in this notebook are derived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d2d97-960c-4177-9572-87f5e9c87f8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514fd715-42d0-4564-a306-963494f32cbf",
   "metadata": {},
   "source": [
    "PyTorch's `tensor`s are similar to NumPy's `ndarray`s that you've been using throughout the first half of this course. If you come from a background in mathematics or physics you might be expecting something with stricter definitions. In PyTorch and many other deep learning frameworks, however, \"tensor\" is just another name for multi-dimensional array. They can be used to represent data or model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38974d39-77c6-4735-983b-b6f741a59dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t)\n",
    "print(t.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f045af5-1162-43de-ac71-6a8cd985a27c",
   "metadata": {},
   "source": [
    "You can also convert between NumPy arrays and PyTorch tensors. This is helpful when you loaded your data as NumPy arrays or when you want to visualize PyTorch tensors with libraries that work with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ebf590-e7d7-430f-9c68-789da21da925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "d = t.numpy()\n",
    "print(d)\n",
    "print(d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69199d2a-e86f-43a1-9c9a-e6f881ddbeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "t = torch.from_numpy(d)\n",
    "print(t)\n",
    "print(t.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db7d3e-2da3-40cb-8a88-c3d98dfd235b",
   "metadata": {},
   "source": [
    "Tensors can be copied between CPUs and GPUs (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d1743a-26e5-416a-b2c2-e55d903c2738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], device='cuda:0')\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    t_gpu = t.to('cuda:0')\n",
    "    print(t_gpu)\n",
    "    t = t_gpu.to('cpu')\n",
    "    print(t)\n",
    "else:\n",
    "    print('CUDA is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f615c-8e48-4f19-ac13-c6eea9204c52",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a5283-e302-4ed5-8b80-7b39953ce210",
   "metadata": {},
   "source": [
    "PyTorch tensors are useful not only because they can be used on GPU, but also because PyTorch has automatic differentiation funtionalities built for them.\n",
    "\n",
    "The most typical way of training a deep learning model is the following:\n",
    "- The model is defined and the model parameters are initialized.\n",
    "- A loss function is define to measure how good the model's predictions are.\n",
    "- During training, for each batch of training samples, do the following:\n",
    "    - Forward pass: Training inputs are fed into the network to generate predictions.\n",
    "    - Calculate the loss given the predictions.\n",
    "    - Backward pass: Calculate the gradients of the loss with respect to the parameters.\n",
    "    - Update model parameters based on the gradients.\n",
    "\n",
    "(We will cover the details in class in the next module. Here, you just need to know that it's essential to calculate these gradients.)\n",
    "\n",
    "When you define your model parameters and all the computations, PyTorch creates a computational graph and uses its built-in differentitaion engine called `torch.autograd` to calculate gradients for the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1860e2dc-c445-4e0e-99dc-44bee7714dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0725]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.2, 1.1, 0.5, 1.6]])  # input\n",
    "y = torch.tensor([[3.]])  # label\n",
    "\n",
    "w = torch.randn(4, 1, requires_grad=True)  # layer weight\n",
    "b = torch.randn(1, requires_grad=True)  # layer bias\n",
    "z = x @ w + b\n",
    "\n",
    "loss = (y - z)**2\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b32f5-1bf5-40ba-8270-4dd184cecdf6",
   "metadata": {},
   "source": [
    "Gradients can be calculated by calling `loss.backward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f96368-046e-45eb-a98f-6225c68220c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6462],\n",
      "        [0.5924],\n",
      "        [0.2693],\n",
      "        [0.8616]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41bcd6b-aa6f-4dad-8922-f8abf4c63512",
   "metadata": {},
   "source": [
    "We can [derive](http://cs231n.stanford.edu/handouts/linear-backprop.pdf) the gradients by hand and verify that this is indeed what `torch.autograd` returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054daac9-b07b-44d7-aaa9-a07e26888739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6462],\n",
      "        [0.5924],\n",
      "        [0.2693],\n",
      "        [0.8616]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    w_grad = x.T @ (2 * (z - y))\n",
    "print(w_grad)\n",
    "print(torch.allclose(w.grad, w_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b4774-7862-437c-b756-2da5263f35da",
   "metadata": {},
   "source": [
    "## Layers and Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b763142-a5b1-414f-bcf7-f1364f5428b2",
   "metadata": {},
   "source": [
    "PyTorch has the [`torch.nn`](https://pytorch.org/docs/stable/nn.html) namespace that provides many commonly used neural net layers so you don't have to write out all the computations in the model and manually manage all the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89376b60-cd13-4a97-9277-d61836133064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "layer = nn.Linear(4, 1)\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401aa2bc-a7b9-43f8-a0fa-28202fb87d78",
   "metadata": {},
   "source": [
    "Here is the same example we saw in the last section, but using the layer instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed410e7-d960-4604-99cf-0f20e9f5e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.5471]], grad_fn=<PowBackward0>)\n",
      "tensor([[-6.5933],\n",
      "        [-6.0438],\n",
      "        [-2.7472],\n",
      "        [-8.7910]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.2, 1.1, 0.5, 1.6]])  # input\n",
    "y = torch.tensor([[3.]])  # label\n",
    "\n",
    "z = layer(x)\n",
    "\n",
    "loss = (y - z)**2\n",
    "print(loss)\n",
    "\n",
    "layer.zero_grad()\n",
    "loss.backward()\n",
    "print(layer._parameters['weight'].grad.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542c7ce-eb24-4ce5-9da9-e5511da20baa",
   "metadata": {},
   "source": [
    "Layers in `torch.nn` are subclasses of `torch.nn.Module`. When building your own networks, you often want multiple layers, and you can define them in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39491c20-9e22-42cf-9e9a-3e69386962cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a76d59-9b3f-4ad4-baa8-8b3c40896e11",
   "metadata": {},
   "source": [
    "Notice that the network itself is also a subclass of `nn.Module`. PyTorch has many functionalities built around the abstraction of modules, such as integration with optimizers and functionalities to save and restore models. Check out more details [here](https://pytorch.org/docs/stable/notes/modules.html) if you are interested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
